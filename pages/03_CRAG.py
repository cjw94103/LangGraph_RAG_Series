import streamlit as st
import time
import uuid

from utils.helper import openai_api_setting, make_uploadfile_to_retriever, load_string_from_txt, tavily_api_setting
from langchain_openai import OpenAIEmbeddings

from langchain_core.runnables import RunnableConfig
from core.crag import CRAG

from utils.web_design import set_web_design
from utils.upload_utils import save_cache_files

## Web Design
set_web_design(page_title="LangGraph RAG Algorithms", 
               page_icon="./logo_imgs/logo.png", 
               title="Corrective RAG", 
               caption="",
               logo_path = './logo_imgs/logo_page.png')

with st.sidebar:
    "ğŸ˜€ ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ì • ğŸ˜€"

    st.divider()
    llm_name = st.selectbox("ì‚¬ìš© í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.", ("gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"))
    st.divider()
    temperature = st.slider("Temperature íŒŒë¼ë¯¸í„° ì„¤ì •", min_value=0.0, max_value=1.0, value=0.2, step=0.1)
    top_k = st.slider("Top-k íŒŒë¼ë¯¸í„° (ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜) ì„¤ì •", min_value=1, max_value=10, value=3, step=1)
    st.divider()
    uploaded_files = st.file_uploader("ğŸ“„íŒŒì¼ ì—…ë¡œë“œğŸ“„", type=['docx', 'pdf', 'hwpx', 'txt', 'md'], accept_multiple_files=True)
    st.divider()
    openai_api_key = st.text_input("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    tavily_api_key = st.text_input("Tavily Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    st.divider()

    process = st.button("Process")
    st.divider()
    conver_init = st.button("ëŒ€í™” ì´ˆê¸°í™”", type="primary")

    ## save global parameter
    st.session_state.llm_name = llm_name
    st.session_state.temperature = temperature
    st.session_state.top_k = top_k

if process:
    ## OpenAI API ì„¤ì •
    openai_api_setting(openai_api_key)
    tavily_api_setting(tavily_api_key)

    ## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    st.session_state.llm_sys_prompt = load_string_from_txt("./sys_prompt_hub/03_crag/03_llm_answer_system_message.txt")

    ## ì—…ë¡œë“œ íŒŒì¼ ì²˜ë¦¬
    with st.spinner("ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...", show_time=True):
        filepaths = save_cache_files(directory="./user_upload_files", uploaded_files=uploaded_files)

    # dense retriever
    retriever = make_uploadfile_to_retriever(filepaths=filepaths, 
                                             embedding_function=OpenAIEmbeddings(model="text-embedding-3-small"),
                                             embedding_dim=1536,
                                             top_k=st.session_state.top_k)

    st.session_state.retriever = retriever

    ## graph ìƒì„±
    thread_id = str(uuid.uuid4())
    graph_ins = CRAG(retriever=st.session_state.retriever, top_k=st.session_state.top_k)
    app = graph_ins.make_app_graph()

    st.session_state.thread_id = thread_id
    st.session_state.app = app

if conver_init:
    st.session_state.pop('app', None)
    st.session_state.pop('messages', None)

    thread_id = str(uuid.uuid4())
    graph_ins = CRAG(retriever=st.session_state.retriever, top_k=st.session_state.top_k)
    app = graph_ins.make_app_graph()

    st.session_state.thread_id = thread_id
    st.session_state.app = app

## ëŒ€í™” ì‹œì‘
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if question := st.chat_input():
    with st.chat_message("user"):
        st.session_state.messages.append({"role": "user", "content": question})
        st.write(question)

    with st.chat_message("assistant"):
        config = RunnableConfig(recursion_limit=20, configurable={"thread_id": st.session_state.thread_id, 
                                                                  "answer_llm_name" : st.session_state.llm_name,
                                                                  "answer_llm_temperature" : st.session_state.temperature})
        
        inputs = {"question" : question,
                 "answer" : "",
                 "result_chunks" : [],
                 "chat_history" : [],
                 "llm_sys_prompt" : st.session_state.llm_sys_prompt}

        output_str = ""
        def stream_data():
            global output_str
            for chunk_msg, metadata  in st.session_state.app.stream(inputs, config, stream_mode="messages"):
                if metadata["langgraph_node"] == "llm_answer":
                    if chunk_msg.content:
                        output_str += chunk_msg.content
                        yield chunk_msg.content
                        time.sleep(0.02)
                        
        st.write_stream(stream_data)
        st.session_state.messages.append({"role": "assistant", "content": output_str})